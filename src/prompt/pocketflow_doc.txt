<document>
# Echo example
from pocketflow_v2 import Node, Flow
from utils.call_llm import call_llm

class AnswerNode(Node):
    def prep(self, shared):
        # Read question from shared
        return shared["question"]
    
    def exec(self, question):
        # Echoing the question
        return f"Echo: {question}"
    
    def post(self, shared, prep_res, exec_res):
        # Store the answer in shared
        shared["answer"] = exec_res

answer_node = AnswerNode()
qa_flow = Flow(start=answer_node)

shared = {
    "question": "In one sentence, what's the end of universe?",
    "answer": None
}
qa_flow.run(shared)
print("Question:", shared["question"])
print("Answer:", shared["answer"])

# Node

A **Node** is the smallest building block. Each Node has 3 steps prep->exec->post:

1. prep(shared)
   - **Read and preprocess data** from shared store. 
   - Examples: *query DB, read files, or serialize data into a string*.
   - Return prep_res, which is used by exec() and post().

2. exec(prep_res)
   - **Execute compute logic**, with optional retries and error handling (below).
   - Examples: *(mostly) LLM calls, remote APIs, tool use*.
   - ⚠️ This shall be only for compute and **NOT** access shared.
   - ⚠️ If retries enabled, ensure idempotent implementation.
   - Return exec_res, which is passed to post().

3. post(shared, prep_res, exec_res)
   - **Postprocess and write data** back to shared.
   - Examples: *update DB, change states, log results*.
   - **Decide the next action** by returning a *string* (action = "default" if *None*).

> **Why 3 steps?** To enforce the principle of *separation of concerns*. The data storage and data processing are operated separately.
>
> All steps are *optional*. E.g., you can only implement prep and post if you just need to process data.
{: .note }

### Fault Tolerance & Retries

You can **retry** exec() if it raises an exception via two parameters when define the Node:

- max_retries (int): Max times to run exec(). The default is 1 (**no** retry).
- wait (int): The time to wait (in **seconds**) before next retry. By default, wait=0 (no waiting). 
wait is helpful when you encounter rate-limits or quota errors from your LLM provider and need to back off.

python 
my_node = SummarizeFile(max_retries=3, wait=10)


When an exception occurs in exec(), the Node automatically retries until:

- It either succeeds, or
- The Node has retried max_retries - 1 times already and fails on the last attempt.

You can get the current retry times (0-based) from self.cur_retry.

python 
class RetryNode(Node):
    def exec(self, prep_res):
        print(f"Retry {self.cur_retry} times")
        raise Exception("Failed")


### Graceful Fallback

To **gracefully handle** the exception (after all retries) rather than raising it, override:

python 
def exec_fallback(self, prep_res, exc):
    raise exc


By default, it just re-raises exception. But you can return a fallback result instead, which becomes the exec_res passed to post().

### Example: Summarize file

python 
class SummarizeFile(Node):
    def prep(self, shared):
        return shared["data"]

    def exec(self, prep_res):
        if not prep_res:
            return "Empty file content"
        prompt = f"Summarize this text in 10 words: {prep_res}"
        summary = call_llm(prompt)  # might fail
        return summary

    def exec_fallback(self, prep_res, exc):
        # Provide a simple fallback instead of crashing
        return "There was an error processing your request."

    def post(self, shared, prep_res, exec_res):
        shared["summary"] = exec_res
        # Return "default" by not returning

summarize_node = SummarizeFile(max_retries=3)

# node.run() calls prep->exec->post
# If exec() fails, it retries up to 3 times before calling exec_fallback()
action_result = summarize_node.run(shared)

print("Action returned:", action_result)  # "default"
print("Summary stored:", shared["summary"])

# Batch

**Batch** makes it easier to handle large inputs in one Node or **rerun** a Flow multiple times. Example use cases:
- **Chunk-based** processing (e.g., splitting large texts).
- **Iterative** processing over lists of input items (e.g., user queries, files, URLs).

## 1. BatchNode

A **BatchNode** extends `Node` but changes `prep()` and `exec()`:

- **`prep(shared)`**: returns an **iterable** (e.g., list, generator).
- **`exec(item)`**: called **once** per item in that iterable.
- **`post(shared, prep_res, exec_res_list)`**: after all items are processed, receives a **list** of results (`exec_res_list`) and returns an **Action**.


### Example: Summarize a Large File

```python
class MapSummaries(BatchNode):
    def prep(self, shared):
        # Suppose we have a big file; chunk it
        content = shared["data"]
        chunk_size = 10000
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
        return chunks

    def exec(self, chunk):
        prompt = f"Summarize this chunk in 10 words: {chunk}"
        summary = call_llm(prompt)
        return summary

    def post(self, shared, prep_res, exec_res_list):
        combined = "\n".join(exec_res_list)
        shared["summary"] = combined
        return "default"

map_summaries = MapSummaries()
flow = Flow(start=map_summaries)
flow.run(shared)
```

---

## 2. BatchFlow

A **BatchFlow** runs a **Flow** multiple times, each time with different `params`. Think of it as a loop that replays the Flow for each parameter set.

### Key Differences from BatchNode

**Important**: Unlike BatchNode, which processes items and modifies the shared store:

1. BatchFlow returns **parameters to pass to the child Flow**, not data to process
2. These parameters are accessed in child nodes via `self.params`, not from the shared store
3. Each child Flow runs independently with a different set of parameters
4. Child nodes can be regular Nodes, not BatchNodes (the batching happens at the Flow level)

### Example: Summarize Many Files

```python
class SummarizeAllFiles(BatchFlow):
    def prep(self, shared):
        # IMPORTANT: Return a list of param dictionaries (not data for processing)
        filenames = list(shared["data"].keys())  # e.g., ["file1.txt", "file2.txt", ...]
        return [{"filename": fn} for fn in filenames]

# Child node that accesses filename from params, not shared store
class LoadFile(Node):
    def prep(self, shared):
        # Access filename from params (not from shared)
        filename = self.params["filename"]  # Important! Use self.params, not shared
        return filename
        
    def exec(self, filename):
        with open(filename, 'r') as f:
            return f.read()
            
    def post(self, shared, prep_res, exec_res):
        # Store file content in shared
        shared["current_file_content"] = exec_res
        return "default"

# Summarize node that works on the currently loaded file
class Summarize(Node):
    def prep(self, shared):
        return shared["current_file_content"]
        
    def exec(self, content):
        prompt = f"Summarize this file in 50 words: {content}"
        return call_llm(prompt)
        
    def post(self, shared, prep_res, exec_res):
        # Store summary in shared, indexed by current filename
        filename = self.params["filename"]  # Again, using params
        if "summaries" not in shared:
            shared["summaries"] = {}
        shared["summaries"][filename] = exec_res
        return "default"

# Create a per-file flow
load_file = LoadFile()
summarize = Summarize()
load_file >> summarize
summarize_file = Flow(start=load_file)

# Wrap in a BatchFlow to process all files
summarize_all_files = SummarizeAllFiles(start=summarize_file)
summarize_all_files.run(shared)
```

### Under the Hood
1. `prep(shared)` in the BatchFlow returns a list of param dicts—e.g., `[{"filename": "file1.txt"}, {"filename": "file2.txt"}, ...]`.
2. The **BatchFlow** loops through each dict. For each one:
   - It merges the dict with the BatchFlow's own `params` (if any): `{**batch_flow.params, **dict_from_prep}`
   - It calls `flow.run(shared)` using the merged parameters
   - **IMPORTANT**: These parameters are passed to the child Flow's nodes via `self.params`, NOT via the shared store
3. This means the sub-Flow is run **repeatedly**, once for every param dict, with each node in the flow accessing the parameters via `self.params`.

---

## 3. Nested or Multi-Level Batches

You can nest a **BatchFlow** in another **BatchFlow**. For instance:
- **Outer** batch: returns a list of directory param dicts (e.g., `{"directory": "/pathA"}`, `{"directory": "/pathB"}`, ...).
- **Inner** batch: returning a list of per-file param dicts.

At each level, **BatchFlow** merges its own param dict with the parent’s. By the time you reach the **innermost** node, the final `params` is the merged result of **all** parents in the chain. This way, a nested structure can keep track of the entire context (e.g., directory + file name) at once.

```python

class FileBatchFlow(BatchFlow):
    def prep(self, shared):
        # Access directory from params (set by parent)
        directory = self.params["directory"]
        # e.g., files = ["file1.txt", "file2.txt", ...]
        files = [f for f in os.listdir(directory) if f.endswith(".txt")]
        return [{"filename": f} for f in files]

class DirectoryBatchFlow(BatchFlow):
    def prep(self, shared):
        directories = [ "/path/to/dirA", "/path/to/dirB"]
        return [{"directory": d} for d in directories]

# The actual processing node
class ProcessFile(Node):
    def prep(self, shared):
        # Access both directory and filename from params
        directory = self.params["directory"]  # From outer batch
        filename = self.params["filename"]    # From inner batch
        full_path = os.path.join(directory, filename)
        return full_path
        
    def exec(self, full_path):
        # Process the file...
        return f"Processed {full_path}"
        
    def post(self, shared, prep_res, exec_res):
        # Store results, perhaps indexed by path
        if "results" not in shared:
            shared["results"] = {}
        shared["results"][prep_res] = exec_res
        return "default"

# Set up the nested batch structure
process_node = ProcessFile()
inner_flow = FileBatchFlow(start=process_node)
outer_flow = DirectoryBatchFlow(start=inner_flow)

# Run it
outer_flow.run(shared)
```

# Flow

A **Flow** orchestrates a graph of Nodes. You can chain Nodes in a sequence or create branching depending on the **Actions** returned from each Node's post().

## 1. Action-based Transitions

Each Node's post() returns an **Action** string. By default, if post() doesn't return anything, we treat that as "default".

You define transitions with the syntax:

1. **Basic default transition**: node_a >> node_b
  This means if node_a.post() returns "default", go to node_b. 
  (Equivalent to node_a - "default" >> node_b)

2. **Named action transition**: node_a - "action_name" >> node_b
  This means if node_a.post() returns "action_name", go to node_b.

It's possible to create loops, branching, or multi-step flows.

## 2. Creating a Flow

A **Flow** begins with a **start** node. You call Flow(start=some_node) to specify the entry point. When you call flow.run(shared), it executes the start node, looks at its returned Action from post(), follows the transition, and continues until there's no next node.

### Example: Simple Sequence

Here's a minimal flow of two nodes in a chain:

python
node_a >> node_b
flow = Flow(start=node_a)
flow.run(shared)


- When you run the flow, it executes node_a.  
- Suppose node_a.post() returns "default".  
- The flow then sees "default" Action is linked to node_b and runs node_b.  
- node_b.post() returns "default" but we didn't define node_b >> something_else. So the flow ends there.

### Example: Branching & Looping

Here's a simple expense approval flow that demonstrates branching and looping. The ReviewExpense node can return three possible Actions:

- "approved": expense is approved, move to payment processing
- "needs_revision": expense needs changes, send back for revision 
- "rejected": expense is denied, finish the process

We can wire them like this:

python
# Define the flow connections
review - "approved" >> payment        # If approved, process payment
review - "needs_revision" >> revise   # If needs changes, go to revision
review - "rejected" >> finish         # If rejected, finish the process

revise >> review   # After revision, go back for another review
payment >> finish  # After payment, finish the process

flow = Flow(start=review)


Let's see how it flows:

1. If review.post() returns "approved", the expense moves to the payment node
2. If review.post() returns "needs_revision", it goes to the revise node, which then loops back to review
3. If review.post() returns "rejected", it moves to the finish node and stops

mermaid
flowchart TD
    review[Review Expense] -->|approved| payment[Process Payment]
    review -->|needs_revision| revise[Revise Report]
    review -->|rejected| finish[Finish Process]

    revise --> review
    payment --> finish


### Running Individual Nodes vs. Running a Flow

- node.run(shared): Just runs that node alone (calls prep->exec->post()), returns an Action. 
- flow.run(shared): Executes from the start node, follows Actions to the next node, and so on until the flow can't continue.

> node.run(shared) **does not** proceed to the successor.
> This is mainly for debugging or testing a single node.
> 
> Always use flow.run(...) in production to ensure the full pipeline runs correctly.
{: .warning }

## 3. Nested Flows

A **Flow** can act like a Node, which enables powerful composition patterns. This means you can:

1. Use a Flow as a Node within another Flow's transitions.  
2. Combine multiple smaller Flows into a larger Flow for reuse.  
3. Node params will be a merging of **all** parents' params.

### Flow's Node Methods

A **Flow** is also a **Node**, so it will run prep() and post(). However:

- It **won't** run exec(), as its main logic is to orchestrate its nodes.
- post() always receives None for exec_res and should instead get the flow execution results from the shared store.

### Basic Flow Nesting

Here's how to connect a flow to another node:

python
# Create a sub-flow
node_a >> node_b
subflow = Flow(start=node_a)

# Connect it to another node
subflow >> node_c

# Create the parent flow
parent_flow = Flow(start=subflow)


When parent_flow.run() executes:
1. It starts subflow
2. subflow runs through its nodes (node_a->node_b)
3. After subflow completes, execution continues to node_c

### Example: Order Processing Pipeline

Here's a practical example that breaks down order processing into nested flows:

python
# Payment processing sub-flow
validate_payment >> process_payment >> payment_confirmation
payment_flow = Flow(start=validate_payment)

# Inventory sub-flow
check_stock >> reserve_items >> update_inventory
inventory_flow = Flow(start=check_stock)

# Shipping sub-flow
create_label >> assign_carrier >> schedule_pickup
shipping_flow = Flow(start=create_label)

# Connect the flows into a main order pipeline
payment_flow >> inventory_flow >> shipping_flow

# Create the master flow
order_pipeline = Flow(start=payment_flow)

# Run the entire pipeline
order_pipeline.run(shared_data)


This creates a clean separation of concerns while maintaining a clear execution path:

mermaid
flowchart LR
    subgraph order_pipeline[Order Pipeline]
        subgraph paymentFlow["Payment Flow"]
            A[Validate Payment] --> B[Process Payment] --> C[Payment Confirmation]
        end

        subgraph inventoryFlow["Inventory Flow"]
            D[Check Stock] --> E[Reserve Items] --> F[Update Inventory]
        end

        subgraph shippingFlow["Shipping Flow"]
            G[Create Label] --> H[Assign Carrier] --> I[Schedule Pickup]
        end

        paymentFlow --> inventoryFlow
        inventoryFlow --> shippingFlow
    end
</document>

# Workflow Design pattern
## Prompt Chaining
Break tasks into sequential steps, where each LLM call processes the output of the previous one.

python
# Define the nodes
outline_creator = CreateOutlineNode()
outline_validator = ValidateOutlineNode()
document_writer = WriteDocumentNode()

# Chain them together
outline_creator >> outline_validator
outline_validator - "valid" >> document_writer
outline_validator - "invalid" >> outline_creator  # Loop back for revision

# Create the flow
document_flow = Flow(start=outline_creator)
This pattern is ideal when a task can be cleanly broken into fixed subtasks, trading latency for higher accuracy.

## Routing
Pattern 2: Routing
Classify input and direct it to specialized paths based on the classification.

python
# Define the nodes
classifier = ClassifyQueryNode()
general_query = GeneralQueryNode()
refund_request = RefundRequestNode()
technical_support = TechnicalSupportNode()

# Set up routing based on classification
classifier - "general" >> general_query
classifier - "refund" >> refund_request
classifier - "technical" >> technical_support

# Create the flow
support_flow = Flow(start=classifier)
This pattern works well for handling distinct categories that are better processed separately.

# Pattern 3: Parallelization
Break a task into independent subtasks that can run in parallel.

python
# Define the nodes
dispatcher = DispatchTaskNode()
content_processor = ProcessContentNode()
safety_checker = CheckSafetyNode()
result_aggregator = AggregateResultsNode()

# Set up parallel paths
dispatcher >> content_processor
dispatcher >> safety_checker
content_processor >> result_aggregator
safety_checker >> result_aggregator

# Create the flow
parallel_flow = Flow(start=dispatcher)

# Pattern 4: Voting (Multiple Attempts)
Run the same task multiple times to get diverse outputs and aggregate them.

python
# Using BatchNode approach
class CodeReviewBatch(BatchNode):
    def prep(self, shared):
        # Return different security aspects to check
        return ["sql_injection", "xss", "authentication", "authorization"]

    def exec(self, security_aspect):
        # Check code for specific security aspect
        # Implementation details omitted
        return result

    def post(self, shared, prep_res, exec_res_list):
        # Aggregate all security reviews
        shared["security_issues"] = [issue for issues in exec_res_list for issue in issues]
        return "default"

# Create the flow
review_node = CodeReviewBatch()
result_processor = ProcessResultsNode()

review_node >> result_processor
code_review_flow = Flow(start=review_node)
This pattern is effective when parallel subtasks improve speed or when multiple perspectives increase confidence.

# Pattern 4: Orchestrator-Workers
A central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes results.

python
# Define the nodes
orchestrator = OrchestratorNode()
file_analyzer = AnalyzeFileNode()
code_generator = GenerateCodeNode()
code_tester = TestCodeNode()
result_synthesizer = SynthesizeResultsNode()

# Set up orchestrator connections
orchestrator - "analyze" >> file_analyzer
orchestrator - "generate" >> code_generator
orchestrator - "test" >> code_tester

# Workers report back to orchestrator
file_analyzer >> orchestrator
code_generator >> orchestrator
code_tester >> orchestrator

# Final synthesis
orchestrator - "synthesize" >> result_synthesizer

# Create the flow
coding_flow = Flow(start=orchestrator)
This pattern suits complex tasks where subtasks can't be predetermined and must be dynamically identified.

# Pattern 5: Evaluator-Optimizer
One LLM generates a response while another evaluates and provides feedback in a loop.

python
# Define the nodes
translator = TranslateTextNode()
evaluator = EvaluateTranslationNode()
refiner = RefineTranslationNode()
finalizer = FinalizeTranslationNode()

# Set up evaluation loop
translator >> evaluator
evaluator - "needs_improvement" >> refiner
refiner >> evaluator  # Loop back for re-evaluation
evaluator - "acceptable" >> finalizer

# Create the flow
translation_flow = Flow(start=translator)
This pattern works when you have clear evaluation criteria and iterative refinement provides measurable value.